{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAB AI for S.E.A. Challenge"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Installing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## important import libraries, if not installed, do sudo pip install <library_name> in terminal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The code requires the safety folder dataset to be in the same directory which contains the notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/amit/Downloads/grab'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.getcwd()+'/safety/labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(\"part-00000-e9445087-aa0a-433b-a7f6-7f4c19d78ad6-c000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.getcwd()+'/../features/')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Reading the dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"part-00000-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\")\n",
    "df2 = pd.read_csv(\"part-00001-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\")\n",
    "df3 = pd.read_csv(\"part-00002-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\")\n",
    "df4 = pd.read_csv(\"part-00003-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\")\n",
    "df5 = pd.read_csv(\"part-00004-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\")\n",
    "df6 = pd.read_csv(\"part-00005-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\")\n",
    "df7 = pd.read_csv(\"part-00006-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\")\n",
    "df8 = pd.read_csv(\"part-00007-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\")\n",
    "df9 = pd.read_csv(\"part-00008-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\")\n",
    "df10 = pd.read_csv(\"part-00009-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Created a absolute acceleration variable which is basically the vector sum of acceleration in x and y direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['absolute_acc'] = np.sqrt(features_df['acceleration_x']**2+features_df['acceleration_z']**2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# There were minimum 120 time slices of data point for each booking number. So, I sort the data points first according to their booking ID (i.e, same booking ID are grouped together) then time slices with same booking ID are sorted in descending order of their absolute acceleration (sqrt((acc_x)**2+(acc_y)**2)). Top 120 data points are selected for training as higher absolute acceleration datapoints have higher chance to decide the safety of the trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.sort_values(['bookingID', 'absolute_acc'], ascending=[True, False],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Preprocessing the data to normalise all the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "    \n",
    "features_df[['acceleration_x','acceleration_y','acceleration_z','gyro_x','gyro_y','gyro_z','Speed','absolute_acc']] = preprocessing.scale(features_df[['acceleration_x','acceleration_y','acceleration_z','gyro_x','gyro_y','gyro_z','Speed','absolute_acc']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = np.zeros(((np.size(labels_df['label'])),120,8))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Keeping acceleration in all axes, gyro readings in all axes, speed and absolute acceleration as 8 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "for i in labels_df['bookingID']:\n",
    "    a[c][:][:]=features_df[features_df['bookingID']==i][['acceleration_x','acceleration_y','acceleration_z','gyro_x','gyro_y','gyro_z','Speed','absolute_acc']][0:120]\n",
    "    c=c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amit/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Doing a train test split of 75 - 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(a, labels_df['label'], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence classification using Convolutional and Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The problem can be treated as classification of time sequence of telemantics data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 118, 30)           750       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 59, 30)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 53,251\n",
      "Trainable params: 53,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/25\n",
      "15013/15013 [==============================] - 35s 2ms/step - loss: 0.5510 - acc: 0.7557\n",
      "Epoch 2/25\n",
      "15013/15013 [==============================] - 7s 482us/step - loss: 0.5409 - acc: 0.7614\n",
      "Epoch 3/25\n",
      "15013/15013 [==============================] - 7s 472us/step - loss: 0.5349 - acc: 0.7630\n",
      "Epoch 4/25\n",
      "15013/15013 [==============================] - 7s 477us/step - loss: 0.5333 - acc: 0.7633\n",
      "Epoch 5/25\n",
      "15013/15013 [==============================] - 7s 487us/step - loss: 0.5314 - acc: 0.7639\n",
      "Epoch 6/25\n",
      "15013/15013 [==============================] - 8s 512us/step - loss: 0.5299 - acc: 0.7649\n",
      "Epoch 7/25\n",
      "15013/15013 [==============================] - 8s 515us/step - loss: 0.5286 - acc: 0.7647\n",
      "Epoch 8/25\n",
      "15013/15013 [==============================] - 8s 530us/step - loss: 0.5267 - acc: 0.7663\n",
      "Epoch 9/25\n",
      "15013/15013 [==============================] - 8s 522us/step - loss: 0.5244 - acc: 0.7663\n",
      "Epoch 10/25\n",
      "15013/15013 [==============================] - 8s 518us/step - loss: 0.5229 - acc: 0.7670\n",
      "Epoch 11/25\n",
      "15013/15013 [==============================] - 8s 521us/step - loss: 0.5212 - acc: 0.7676\n",
      "Epoch 12/25\n",
      "15013/15013 [==============================] - 8s 529us/step - loss: 0.5224 - acc: 0.7679\n",
      "Epoch 13/25\n",
      "15013/15013 [==============================] - 8s 514us/step - loss: 0.5203 - acc: 0.7676\n",
      "Epoch 14/25\n",
      "15013/15013 [==============================] - 8s 527us/step - loss: 0.5175 - acc: 0.7673\n",
      "Epoch 15/25\n",
      "15013/15013 [==============================] - 8s 518us/step - loss: 0.5158 - acc: 0.7683\n",
      "Epoch 16/25\n",
      "15013/15013 [==============================] - 8s 530us/step - loss: 0.5147 - acc: 0.7685\n",
      "Epoch 17/25\n",
      "15013/15013 [==============================] - 8s 535us/step - loss: 0.5120 - acc: 0.7691\n",
      "Epoch 18/25\n",
      "15013/15013 [==============================] - 8s 535us/step - loss: 0.5092 - acc: 0.7701\n",
      "Epoch 19/25\n",
      "15013/15013 [==============================] - 8s 526us/step - loss: 0.5071 - acc: 0.7716\n",
      "Epoch 20/25\n",
      "15013/15013 [==============================] - 8s 521us/step - loss: 0.5040 - acc: 0.7729\n",
      "Epoch 21/25\n",
      "15013/15013 [==============================] - 8s 519us/step - loss: 0.5013 - acc: 0.7725\n",
      "Epoch 22/25\n",
      "15013/15013 [==============================] - 8s 525us/step - loss: 0.4977 - acc: 0.7748\n",
      "Epoch 23/25\n",
      "15013/15013 [==============================] - 8s 527us/step - loss: 0.4954 - acc: 0.7767\n",
      "Epoch 24/25\n",
      "15013/15013 [==============================] - 8s 518us/step - loss: 0.4905 - acc: 0.7773\n",
      "Epoch 25/25\n",
      "15013/15013 [==============================] - 8s 528us/step - loss: 0.4861 - acc: 0.7777\n",
      "Accuracy: 75.48%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(input_shape=(120,8),filters=30, kernel_size=3,activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=25, batch_size=100)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7548451548451548"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred[y_pred <= 0.5] = 0\n",
    "y_pred[y_pred > 0.5] = 1\n",
    "accuracy_score(y_test,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
